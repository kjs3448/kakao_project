{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load\n",
    " - 전처리한 데이터 load. training과 test set으로 구분\n",
    " - 전처리한 데이터는 연속형 혹은 binary의 선택형 데이터로만 이루어진다. 범주형 데이터는 더미변수를 적용하여 변환 시켜 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt(\"data_for_learning_float_2.csv\", delimiter=\",\", dtype=np.float32)\n",
    "x_train = xy[1:30000,0:-1]\n",
    "y_train = xy[1:30000,[-1]]\n",
    "x_test = xy[30001:40000,0:-1]\n",
    "y_test = xy[30001:40000,[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification 정의\n",
    "- Logistic Classification을 사용함\n",
    "  : Y값이 0과 1로 표현할 수 있는 on/off 변수이기 때문에 logistic을 사용\n",
    "  : linear regression 등은 outlier에 따라 오동작할 확률이 높음\n",
    "\n",
    "- Logistic Regression의 cost function을 수식으로 표현하고 (hypythesis) 이를 최소화 시키는 cost function을 만듦 (cost)\n",
    "\n",
    "- Learning Rate를 적당히 조절함\n",
    "  : input data의 스케일이 다를 경우 결과값이 발산할 수 있음\n",
    "  : input data는 0~1 사이의 표준화된 데이터로 발산할 확률은 적지만, 목적함수의 최소값을 찾을 수 있도록 loop를 많이 돌려야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholders for a tensor that will be always fed\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28]) # input 1~28\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) # input 1\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28,1]), name='weight') #variable 28개\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias') #bias 1개\n",
    "\n",
    "# Hypothesis using sigmoid:\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test\n",
    " - train을 feed하고 Test case에 적용해서 accuracy를 뽑아냄, train의 결과가 수렴하는지 확인하기 위해 print를 넣음. \n",
    " - 실험결과 약 20000번의 iteration 후 수렴 >> 테스트마다 수치가 약간씩 변화하지만 Accuracy 약 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Training: [cost =  0.3724904  accuracy =  0.86146206 ]\n",
      "Test: [cost =  0.89168197  accuracy =  0.6791679 ]\n",
      "Step:  2000\n",
      "Training: [cost =  0.34775147  accuracy =  0.89076304 ]\n",
      "Test: [cost =  0.9268802  accuracy =  0.6951695 ]\n",
      "Step:  4000\n",
      "Training: [cost =  0.33599836  accuracy =  0.9027301 ]\n",
      "Test: [cost =  0.950334  accuracy =  0.7038704 ]\n",
      "Step:  6000\n",
      "Training: [cost =  0.32908818  accuracy =  0.90833026 ]\n",
      "Test: [cost =  0.9638703  accuracy =  0.7071707 ]\n",
      "Step:  8000\n",
      "Training: [cost =  0.32423407  accuracy =  0.9115971 ]\n",
      "Test: [cost =  0.9702187  accuracy =  0.70777076 ]\n",
      "Step:  10000\n",
      "Training: [cost =  0.32035497  accuracy =  0.9137638 ]\n",
      "Test: [cost =  0.9715647  accuracy =  0.7087709 ]\n",
      "Step:  12000\n",
      "Training: [cost =  0.31699547  accuracy =  0.9154305 ]\n",
      "Test: [cost =  0.96950674  accuracy =  0.7087709 ]\n",
      "Step:  14000\n",
      "Training: [cost =  0.31394926  accuracy =  0.9170639 ]\n",
      "Test: [cost =  0.96516955  accuracy =  0.7093709 ]\n",
      "Step:  16000\n",
      "Training: [cost =  0.311116  accuracy =  0.9184306 ]\n",
      "Test: [cost =  0.9593398  accuracy =  0.70967096 ]\n",
      "Step:  18000\n",
      "Training: [cost =  0.30844992  accuracy =  0.919464 ]\n",
      "Test: [cost =  0.9525856  accuracy =  0.71057105 ]\n",
      "Step:  20000\n",
      "Training: [cost =  0.3059262  accuracy =  0.920264 ]\n",
      "Test: [cost =  0.94529915  accuracy =  0.7116712 ]\n",
      "\n",
      "Hypothesis:  [[0.12672418]\n",
      " [0.5330555 ]\n",
      " [0.01059852]\n",
      " ...\n",
      " [0.03233078]\n",
      " [0.0043967 ]\n",
      " [0.00325758]]\n",
      "\n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "TEST Accuracy:  0.7116712\n"
     ]
    }
   ],
   "source": [
    "# Launch Graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    feed_train = {X:x_train, Y:y_train} # training\n",
    "    feed_test = {X:x_test, Y:y_test} # training\n",
    "    for step in range(20001):\n",
    "        sess.run(train, feed_dict=feed_train)\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: \", step)\n",
    "            c, a = sess.run([cost, accuracy], feed_dict=feed_train)\n",
    "            print(\"Training: [cost = \", c, \" accuracy = \", a, \"]\")\n",
    "            c, a = sess.run([cost, accuracy], feed_dict=feed_test)\n",
    "            print(\"Test: [cost = \", c, \" accuracy = \", a, \"]\")\n",
    "            \n",
    "\n",
    "    # Result : TEST  case\n",
    "    feed = {X: x_test, Y: y_test}\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = feed)\n",
    "    print(\"\\nHypothesis: \", h)\n",
    "    print(\"\\nCorrect (Y): \", c)\n",
    "    print(\"\\nTEST Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
