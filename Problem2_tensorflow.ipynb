{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 : 분류기 만들기\n",
    " - 전처리한 데이터 불러오기. training과 test set으로 구분되어 있음\n",
    "   - Training data set:28000개, Test data set: 12000개\n",
    " - 전처리한 데이터는 연속형, 범주형 데이터로만 이루어진다. 범주형 데이터는 더미변수를 적용하여 변환되어 있음\n",
    "   - 데이터 범위 : 0~1로 표준화\n",
    "   - x2, x3을 더미형으로 추가했기 때문에 변수는 15개에서 28개로 늘어남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train = np.loadtxt(\"data_for_training_v1.csv\", delimiter=\",\", dtype=np.float32)\n",
    "xy_test = np.loadtxt(\"data_for_test_v1.csv\", delimiter=\",\", dtype=np.float32)\n",
    "x_train = xy_train[:,0:-1]\n",
    "y_train = xy_train[:,[-1]]\n",
    "x_test = xy_test[:,0:-1]\n",
    "y_test = xy_test[:,[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification \n",
    "- Logistic Classification 선택 이유\n",
    "  - Y값이 continuous한 경우 linear regression, 여러개 level한 경우는 Softmax 등을 사용할 수 있음\n",
    "  - 허나 Y가 0/1인 경우 상기 방법을 사용할 경우 outlier에 따라 오동작할 확률이 높음\n",
    "  - Y값이 0과 1로 표현할 수 있는 on/off 변수이기 때문에 logistic classification을 사용\n",
    "\n",
    "- Logistic Regression의 cost function을 수식으로 표현하고 (hypythesis) 이를 최소화 시키는 cost function을 만듦 (cost)\n",
    "\n",
    "- Learning Rate를 적당히 조절함\n",
    "  : input data의 스케일이 다를 경우 결과값이 발산할 수 있음\n",
    "  : input data는 0~1 사이의 표준화된 데이터로 발산할 확률은 적지만, 목적함수의 최소값을 찾을 수 있도록 loop를 많이 돌려야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholders for a tensor that will be always fed\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28]) # input 1~28\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) # input 1\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28,1]), name='weight') #variable 28개\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias') #bias 1개\n",
    "\n",
    "# Hypothesis using sigmoid:\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test\n",
    " - train을 feed하고 Test case에 적용해서 accuracy를 뽑아냄, train의 결과가 수렴하는지 확인하기 위해 print를 넣음. \n",
    " - 실험결과 약 30000번의 iteration 후 수렴 >> 테스트마다 수치가 약간씩 변화하지만 Accuracy 약 88%\n",
    " - Iteration 횟수 30000번을 선택한 이유\n",
    "   : cost의 변화량이 줄어드는 시점의 iteration 값을 추출\n",
    "   : 하기 실험의 경우 약 30000번에서부터 cost의 변화량이 0에 가까워진다고 볼 수 있음\n",
    "   : 하기 코드는 50000으로 하였지만 30000으로 해도 비슷한 결과가 도출\n",
    " ## 분류기 정확도 : 약 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "Training: [cost =  2.5414045  accuracy =  0.13432142 ]\n",
      "Test: [cost =  2.5487869  accuracy =  0.13083333 ]\n",
      "Step:  2000\n",
      "Training: [cost =  0.6305362  accuracy =  0.6694643 ]\n",
      "Test: [cost =  0.625946  accuracy =  0.672 ]\n",
      "Step:  4000\n",
      "Training: [cost =  0.46909922  accuracy =  0.8356786 ]\n",
      "Test: [cost =  0.46081176  accuracy =  0.8415 ]\n",
      "Step:  6000\n",
      "Training: [cost =  0.44264165  accuracy =  0.8626071 ]\n",
      "Test: [cost =  0.43329814  accuracy =  0.8666667 ]\n",
      "Step:  8000\n",
      "Training: [cost =  0.4323194  accuracy =  0.868 ]\n",
      "Test: [cost =  0.4227185  accuracy =  0.87233335 ]\n",
      "Step:  10000\n",
      "Training: [cost =  0.42528397  accuracy =  0.87110716 ]\n",
      "Test: [cost =  0.4157366  accuracy =  0.87516665 ]\n",
      "Step:  12000\n",
      "Training: [cost =  0.41932583  accuracy =  0.8723214 ]\n",
      "Test: [cost =  0.4099611  accuracy =  0.87691665 ]\n",
      "Step:  14000\n",
      "Training: [cost =  0.41396046  accuracy =  0.8732143 ]\n",
      "Test: [cost =  0.4048286  accuracy =  0.87766665 ]\n",
      "Step:  16000\n",
      "Training: [cost =  0.40905094  accuracy =  0.8740357 ]\n",
      "Test: [cost =  0.4001661  accuracy =  0.8781667 ]\n",
      "Step:  18000\n",
      "Training: [cost =  0.40454033  accuracy =  0.8749286 ]\n",
      "Test: [cost =  0.39589965  accuracy =  0.8785 ]\n",
      "Step:  20000\n",
      "Training: [cost =  0.40038982  accuracy =  0.87525 ]\n",
      "Test: [cost =  0.391982  accuracy =  0.879 ]\n",
      "Step:  22000\n",
      "Training: [cost =  0.39656642  accuracy =  0.87525 ]\n",
      "Test: [cost =  0.38837713  accuracy =  0.87916666 ]\n",
      "Step:  24000\n",
      "Training: [cost =  0.39304134  accuracy =  0.87535715 ]\n",
      "Test: [cost =  0.385055  accuracy =  0.87916666 ]\n",
      "Step:  26000\n",
      "Training: [cost =  0.38978773  accuracy =  0.8755357 ]\n",
      "Test: [cost =  0.38198966  accuracy =  0.8793333 ]\n",
      "Step:  28000\n",
      "Training: [cost =  0.38678062  accuracy =  0.87560713 ]\n",
      "Test: [cost =  0.37915593  accuracy =  0.8795 ]\n",
      "Step:  30000\n",
      "Training: [cost =  0.38399854  accuracy =  0.87575 ]\n",
      "Test: [cost =  0.37653336  accuracy =  0.87941664 ]\n",
      "Step:  32000\n",
      "Training: [cost =  0.38142076  accuracy =  0.8758214 ]\n",
      "Test: [cost =  0.37410238  accuracy =  0.8795 ]\n",
      "Step:  34000\n",
      "Training: [cost =  0.379029  accuracy =  0.8758571 ]\n",
      "Test: [cost =  0.37184486  accuracy =  0.87958336 ]\n",
      "Step:  36000\n",
      "Training: [cost =  0.37680706  accuracy =  0.8759286 ]\n",
      "Test: [cost =  0.36974666  accuracy =  0.8796667 ]\n",
      "Step:  38000\n",
      "Training: [cost =  0.37473953  accuracy =  0.8759286 ]\n",
      "Test: [cost =  0.3677918  accuracy =  0.87958336 ]\n",
      "Step:  40000\n",
      "Training: [cost =  0.3728133  accuracy =  0.8758571 ]\n",
      "Test: [cost =  0.36596838  accuracy =  0.8796667 ]\n",
      "Step:  42000\n",
      "Training: [cost =  0.37101594  accuracy =  0.8758571 ]\n",
      "Test: [cost =  0.36426574  accuracy =  0.87958336 ]\n",
      "Step:  44000\n",
      "Training: [cost =  0.3693368  accuracy =  0.87564284 ]\n",
      "Test: [cost =  0.36267322  accuracy =  0.8795 ]\n",
      "Step:  46000\n",
      "Training: [cost =  0.3677658  accuracy =  0.87560713 ]\n",
      "Test: [cost =  0.36117998  accuracy =  0.87958336 ]\n",
      "Step:  48000\n",
      "Training: [cost =  0.36629415  accuracy =  0.8755714 ]\n",
      "Test: [cost =  0.35977978  accuracy =  0.87958336 ]\n",
      "Step:  50000\n",
      "Training: [cost =  0.364914  accuracy =  0.8755714 ]\n",
      "Test: [cost =  0.35846493  accuracy =  0.87958336 ]\n",
      "\n",
      "Hypothesis:  [[0.07035263]\n",
      " [0.1350326 ]\n",
      " [0.15671162]\n",
      " ...\n",
      " [0.07543796]\n",
      " [0.06152119]\n",
      " [0.03562672]]\n",
      "\n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "TEST Accuracy:  0.87958336\n"
     ]
    }
   ],
   "source": [
    "# Launch Graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    feed_train = {X:x_train, Y:y_train} # training\n",
    "    feed_test = {X:x_test, Y:y_test} # training\n",
    "    for step in range(50001):\n",
    "        sess.run(train, feed_dict=feed_train)\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: \", step)\n",
    "            c, a = sess.run([cost, accuracy], feed_dict=feed_train)\n",
    "            print(\"Training: [cost = \", c, \" accuracy = \", a, \"]\")\n",
    "            c, a = sess.run([cost, accuracy], feed_dict=feed_test)\n",
    "            print(\"Test: [cost = \", c, \" accuracy = \", a, \"]\")\n",
    "            \n",
    "\n",
    "    # Result : TEST  case\n",
    "    feed = {X: x_test, Y: y_test}\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = feed)\n",
    "    print(\"\\nHypothesis: \", h)\n",
    "    print(\"\\nCorrect (Y): \", c)\n",
    "    print(\"\\nTEST Accuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 (전처리 > 표준화 > Logistic Classification >>> Accuracy 88%)\n",
    " - 데이터 전처리\n",
    "   - 범주형 데이터는 더미변수를 통해 차원을 늘림\n",
    "   - 변수 표준화 (0 ~ 1. min max scaler 사용)\n",
    " - Logistic Classification\n",
    "   - Iteration 횟수\n",
    "     - cost가 수렴하는 시점의 Iteration을 선택\n",
    "   - learning Rate \n",
    "     - training 후 업데이트 될 때 변화 시키는 단위\n",
    "     - 순차적 탐색으로 찾아낼 수 있음\n",
    "     - 모든 변수의 범위를 표준화 시켰기 때문에 NaN이 출력될 확률은 높지 않음\n",
    " - 분류기 정확도: 약 88%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
